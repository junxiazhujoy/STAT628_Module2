{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json \n",
    "import copy\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [] \n",
    "cf = open('LVbars_reviewdf.csv','r')\n",
    "\n",
    "file = csv.DictReader(cf)\n",
    "#print(file.fieldnames)\n",
    "\n",
    "for x in file:\n",
    "    line = {'business_id':x['business_id'],'date':x['date'],'stars':x['stars'],'text':x['text']}\n",
    "    data.append(line)\n",
    "\n",
    "cf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_night=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_night_5star=[]\n",
    "dat_night_4star=[]\n",
    "dat_night_3star=[]\n",
    "dat_night_2star=[]\n",
    "dat_night_1star=[]\n",
    "for i in range(len(dat_night)):\n",
    "    if dat_night[i]['stars']=='5.0':\n",
    "        dat_night_5star.append(dat_night[i])\n",
    "    elif dat_night[i]['stars']=='4.0':\n",
    "        dat_night_4star.append(dat_night[i])\n",
    "    elif dat_night[i]['stars']=='3.0':\n",
    "        dat_night_3star.append(dat_night[i])\n",
    "    elif dat_night[i]['stars']=='2.0':\n",
    "        dat_night_2star.append(dat_night[i])\n",
    "    else:\n",
    "        dat_night_1star.append(dat_night[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewslist = copy.deepcopy(dat_night_1star)\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\$)|(\\*)|(\\%)|(\\=)|(\\#)|(\\&)|(\\~)|(\\@)\")#[^\\P{P}-]+\n",
    "REPLACE_WITH_SPACE = re.compile(\"(\\n)|(\\-)|(\\/)|(\\d)\")\n",
    "\n",
    "def preprocess_reviews(reviews):\n",
    "    reviews = REPLACE_NO_SPACE.sub(\"\", reviews)\n",
    "    reviews = REPLACE_WITH_SPACE.sub(\" \", reviews)\n",
    "    return reviews\n",
    "\n",
    "reviewsTEXT_clean = copy.deepcopy(reviewslist)\n",
    "\n",
    "text_cloud1=''\n",
    "\n",
    "for ind in range(len(reviewslist)):\n",
    "    texts = ''\n",
    "    texts = reviewslist[ind]['text']\n",
    "    texts = texts.lower()\n",
    "    texts = re.sub('isnt','isn\\'t', texts)\n",
    "    texts = re.sub('wasnt','wasn\\'t', texts)\n",
    "    texts = re.sub('werent','weren\\'t', texts)\n",
    "    texts = re.sub('dont','don\\'t', texts)\n",
    "    texts = re.sub('doesnt','doesn\\'t', texts)\n",
    "    texts = re.sub('didnt','didn\\'t', texts)\n",
    "    texts = re.sub('hasnt','hasn\\'t', texts)\n",
    "    texts = re.sub('havent','haven\\'t', texts)\n",
    "    texts = re.sub('hadnt','hadn\\'t', texts)\n",
    "    texts = re.sub('mightnt','mightn\\'t', texts)\n",
    "    texts = re.sub('shouldnt','shouldn\\'t', texts)\n",
    "    texts = re.sub('isn','isn\\'t', texts)\n",
    "    texts = re.sub('wasn','wasn\\'t', texts)\n",
    "    texts = re.sub('weren','weren\\'t', texts)\n",
    "    texts = re.sub('don','don\\'t', texts)\n",
    "    texts = re.sub('doesn','doesn\\'t', texts)\n",
    "    texts = re.sub('didn','didn\\'t', texts)\n",
    "    texts = re.sub('hasn','hasn\\'t', texts)\n",
    "    texts = re.sub('haven','haven\\'t', texts)\n",
    "    texts = re.sub('hadn','hadn\\'t', texts)\n",
    "    texts = re.sub('mightn','mightn\\'t', texts)\n",
    "    texts = re.sub('shouldn','shouldn\\'t', texts)\n",
    "    texts = re.sub('won\\'t','will not', texts)\n",
    "    texts = re.sub('n\\'t',' not', texts)\n",
    "    \n",
    "    #add NOT_\n",
    "    pattern = r'\\.|\\;|\\!|\\?|\\,|\\)|\\(|\\:|\\'|\\\"|\\%'\n",
    "    list_text=re.split(pattern,texts)\n",
    "    \n",
    "    sent=''\n",
    "    for i in range(len(list_text)):\n",
    "        list_text[i] = re.sub('\\+','', list_text[i])\n",
    "        list_text[i] = re.sub('\\*','', list_text[i])\n",
    "        list_text[i] = re.sub('\\$','', list_text[i])\n",
    "        list_text[i] = re.sub('\\[','', list_text[i])\n",
    "        list_text[i] = re.sub('\\]','', list_text[i])\n",
    "        list_text[i] = re.sub('\\%','', list_text[i])\n",
    "        list_text[i] = re.sub('\\\\\\\\',' ', list_text[i])\n",
    "        matchObj1 = re.search(r'(.*)not (.*)',list_text[i])\n",
    "        matchObj2 = re.search(r'(.*) never (.*)',list_text[i])\n",
    "        \n",
    "        if matchObj1 != None :\n",
    "            sub=re.sub(r' ', \" NOT_\", ' '+matchObj1.group(2))\n",
    "            list_text[i] = re.sub(matchObj1.group(2) ,sub, list_text[i])\n",
    "            list_text[i] = re.sub(r'not ','', list_text[i],1)\n",
    "            #list_text[i] = re.sub(',',' ', matchObj1.group(3))\n",
    "            sent = sent + list_text[i] + ' '\n",
    "        \n",
    "        elif matchObj2 != None :\n",
    "            sub=re.sub(r' ', \" NOT_\", ' '+matchObj2.group(2))\n",
    "            list_text[i] = re.sub(matchObj2.group(2) ,sub, list_text[i])\n",
    "            list_text[i] = re.sub(r'never ','', list_text[i],1)\n",
    "            #list_text[i] = re.sub(',',' ', matchObj1.group(3))\n",
    "            sent = sent + list_text[i] + ' '\n",
    "        \n",
    "        else:\n",
    "            sent = sent + list_text[i] + ' '\n",
    "   \n",
    "    #print(sent)\n",
    "    #print(texts)\n",
    "    #print(texts)\n",
    "    text_clean=preprocess_reviews(sent)\n",
    "    #print(text_clean)\n",
    "    \n",
    "    text_cloud1=text_cloud1+text_clean\n",
    "    reviewsTEXT_clean[ind]['text']=text_clean\n",
    "    reviewsTEXT_clean[ind]['text_split']=text_clean.split()\n",
    "    reviewsTEXT_clean[ind]['freq']=nltk.FreqDist(reviewsTEXT_clean[ind]['text_split'])\n",
    "    \n",
    "#reviewsTEXT_clean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select words due to frequency order\n",
    "AllFreq = reviewsTEXT_clean[0]['freq']\n",
    "for ind in range(len(reviewsTEXT_clean)-1):\n",
    "    freqs = reviewsTEXT_clean[ind+1]['freq']\n",
    "    AllFreq.update(freqs)\n",
    "    \n",
    "wordList = sorted(AllFreq.items(),key=lambda item:item[1],reverse=True)\n",
    "\n",
    "#choose the important ones--No.1-100\n",
    "WORDS=[x[0] for x in wordList]\n",
    "WORDS=WORDS[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "negWords=set([\"wouldn't\",'isn','wasn',\"weren't\", \"haven't\", \"hasn't\", \"couldn't\", \"isn't\", 'doesn','hasn',\"mustn't\", 'mightn', 'shan', 'no', \"wasn't\",'aren', \"didn't\", \"hadn't\",\"don't\",'nor',\"won't\",'weren',\"doesn't\",\"needn't\", 'shouldn',\"mightn't\",\"shan't\", 'wouldn',\"shouldn't\",'hadn','would'])\n",
    "d=set(['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z','Ã ','{sigh}','us','place'])\n",
    "stopWords=stop_words-negWords\n",
    "stopWords.update(d)\n",
    "\n",
    "WORDS=[x for x in WORDS if x not in stopWords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_wc={}\n",
    "for i in range(len(wordList)):\n",
    "    if wordList[i][0] in WORDS:\n",
    "        dict_wc[wordList[i][0]]=wordList[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom wordcloud import WordCloud\\nimport matplotlib.pyplot as plt\\n\\nwc = WordCloud(background_color = \\'White\\',width=400,height=300,scale=4,max_font_size=100)\\nwc.generate_from_frequencies(dict_wc)\\n\\nplt.figure(figsize=(20,12), dpi=1200)\\nplt.imshow(wc)\\nplt.axis(\"off\")\\nplt.show()\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wc = WordCloud(background_color = 'White',width=400,height=300,scale=4,max_font_size=100)\n",
    "wc.generate_from_frequencies(dict_wc)\n",
    "\n",
    "plt.figure(figsize=(20,12), dpi=1200)\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=list(dict_wc.keys())\n",
    "frequency=list(dict_wc.values())\n",
    "df_star1 = pd.DataFrame(np.nan,index=range(len(words)), columns=['loc'])\n",
    "df_star1['words']=words\n",
    "df_star1['frequency']=frequency\n",
    "df_star1=df_star1.loc[:,['words','frequency']]\n",
    "from pandas import DataFrame\n",
    "DataFrame.to_csv(df_star1,\"df_star1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
