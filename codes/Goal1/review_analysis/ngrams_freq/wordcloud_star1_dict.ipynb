{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import copy\n",
    "import nltk\n",
    "import stemming\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.stem import PorterStemmer\n",
    "from stemming.porter2 import stem\n",
    "from ngram import *\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [] \n",
    "cf = open('LVbars_reviewdf.csv','r')\n",
    "\n",
    "file = csv.DictReader(cf)\n",
    "#print(file.fieldnames)\n",
    "\n",
    "for x in file:\n",
    "    line = {'business_id':x['business_id'],'date':x['date'],'stars':x['stars'],'text':x['text']}\n",
    "    data.append(line)\n",
    "\n",
    "cf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(data[0:100]).to_csv('LVbars_reviewdf_less.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_night=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_night_5star=[]\n",
    "dat_night_4star=[]\n",
    "dat_night_3star=[]\n",
    "dat_night_2star=[]\n",
    "dat_night_1star=[]\n",
    "for i in range(len(dat_night)):\n",
    "    if dat_night[i]['stars']=='5.0':\n",
    "        dat_night_5star.append(dat_night[i])\n",
    "    elif dat_night[i]['stars']=='4.0':\n",
    "        dat_night_4star.append(dat_night[i])\n",
    "    elif dat_night[i]['stars']=='3.0':\n",
    "        dat_night_3star.append(dat_night[i])\n",
    "    elif dat_night[i]['stars']=='2.0':\n",
    "        dat_night_2star.append(dat_night[i])\n",
    "    else:\n",
    "        dat_night_1star.append(dat_night[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewslist = copy.deepcopy(dat_night_1star)\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\$)|(\\*)|(\\%)|(\\=)|(\\#)|(\\&)|(\\~)|(\\@)\")#[^\\P{P}-]+\n",
    "REPLACE_WITH_SPACE = re.compile(\"(\\n)|(\\-)|(\\/)|(\\d)\")\n",
    "\n",
    "def preprocess_reviews(reviews):\n",
    "    reviews = REPLACE_NO_SPACE.sub(\"\", reviews)\n",
    "    reviews = REPLACE_WITH_SPACE.sub(\" \", reviews)\n",
    "    return reviews\n",
    "\n",
    "reviewsTEXT_clean = copy.deepcopy(reviewslist)\n",
    "\n",
    "for ind in range(len(reviewslist)):\n",
    "    texts = ''\n",
    "    texts = reviewslist[ind]['text']\n",
    "    texts = texts.lower()\n",
    "    texts = re.sub('n\\'t',' not', texts)\n",
    "    texts = re.sub('isnt','isn\\'t', texts)\n",
    "    texts = re.sub('wasnt','wasn\\'t', texts)\n",
    "    texts = re.sub('werent','weren\\'t', texts)\n",
    "    texts = re.sub('dont','don\\'t', texts)\n",
    "    texts = re.sub('doesnt','doesn\\'t', texts)\n",
    "    texts = re.sub('didnt','didn\\'t', texts)\n",
    "    texts = re.sub('hasnt','hasn\\'t', texts)\n",
    "    texts = re.sub('havent','haven\\'t', texts)\n",
    "    texts = re.sub('hadnt','hadn\\'t', texts)\n",
    "    texts = re.sub('mightnt','mightn\\'t', texts)\n",
    "    texts = re.sub('shouldnt','shouldn\\'t', texts)\n",
    "    texts = re.sub('isn','isn\\'t', texts)\n",
    "    texts = re.sub('wasn','wasn\\'t', texts)\n",
    "    texts = re.sub('weren','weren\\'t', texts)\n",
    "    texts = re.sub('don','don\\'t', texts)\n",
    "    texts = re.sub('doesn','doesn\\'t', texts)\n",
    "    texts = re.sub('didn','didn\\'t', texts)\n",
    "    texts = re.sub('hasn','hasn\\'t', texts)\n",
    "    texts = re.sub('haven','haven\\'t', texts)\n",
    "    texts = re.sub('hadn','hadn\\'t', texts)\n",
    "    texts = re.sub('mightn','mightn\\'t', texts)\n",
    "    texts = re.sub('shouldn','shouldn\\'t', texts)\n",
    "    \n",
    "    texts = re.sub('n\\'t',' not', texts)\n",
    "    \n",
    "    #add NOT_\n",
    "    pattern = r'\\.|\\;|\\!|\\?|\\,|\\)|\\(|\\:|\\'|\\\"|\\%'\n",
    "    list_text=re.split(pattern,texts)\n",
    "    \n",
    "    sent=''\n",
    "    for i in range(len(list_text)):\n",
    "        list_text[i] = re.sub('\\+','', list_text[i])\n",
    "        list_text[i] = re.sub('\\*','', list_text[i])\n",
    "        list_text[i] = re.sub('\\$','', list_text[i])\n",
    "        list_text[i] = re.sub('\\[','', list_text[i])\n",
    "        list_text[i] = re.sub('\\]','', list_text[i])\n",
    "        list_text[i] = re.sub('\\%','', list_text[i])\n",
    "        list_text[i] = re.sub('\\\\\\\\',' ', list_text[i])\n",
    "        matchObj1 = re.search(r'(.*)not (.*)',list_text[i])\n",
    "        matchObj2 = re.search(r'(.*) never (.*)',list_text[i])\n",
    "        \n",
    "        if matchObj1 != None :\n",
    "            sub=re.sub(r' ', \" NOT_\", ' '+matchObj1.group(2))\n",
    "            list_text[i] = re.sub(matchObj1.group(2) ,sub, list_text[i])\n",
    "            list_text[i] = re.sub(r'not ','', list_text[i],1)\n",
    "            #list_text[i] = re.sub(',',' ', matchObj1.group(3))\n",
    "            sent = sent + list_text[i] + ' '\n",
    "        \n",
    "        elif matchObj2 != None :\n",
    "            sub=re.sub(r' ', \" NOT_\", ' '+matchObj2.group(2))\n",
    "            list_text[i] = re.sub(matchObj2.group(2) ,sub, list_text[i])\n",
    "            list_text[i] = re.sub(r'never ','', list_text[i],1)\n",
    "            #list_text[i] = re.sub(',',' ', matchObj1.group(3))\n",
    "            sent = sent + list_text[i] + ' '\n",
    "        \n",
    "        else:\n",
    "            sent = sent + list_text[i] + ' '\n",
    "   \n",
    "    #print(sent)\n",
    "    #print(texts)\n",
    "    #print(texts)\n",
    "    text_clean=preprocess_reviews(sent)\n",
    "    #print(text_clean)\n",
    "    st = text_clean\n",
    "    st2 = \" \".join([stem(word) for word in st.split(\" \")])\n",
    "    text_clean = st2\n",
    "    \n",
    "    reviewsTEXT_clean[ind]['text']=text_clean\n",
    "    reviewsTEXT_clean[ind]['text_split']=text_clean.split()\n",
    "    reviewsTEXT_clean[ind]['freq']=nltk.FreqDist(reviewsTEXT_clean[ind]['text_split'])   \n",
    "    reviewsTEXT_clean[ind]['bigrams']=generate_ngrams(text_clean, 2)\n",
    "    reviewsTEXT_clean[ind]['bigrams_freq']=nltk.FreqDist(reviewsTEXT_clean[ind]['bigrams'])\n",
    "    \n",
    "#reviewsTEXT_clean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select words due to frequency order\n",
    "AllFreq = reviewsTEXT_clean[0]['freq']\n",
    "for ind in range(len(reviewsTEXT_clean)-1):\n",
    "    freqs = reviewsTEXT_clean[ind+1]['freq']\n",
    "    AllFreq.update(freqs)\n",
    "    \n",
    "wordList = sorted(AllFreq.items(),key=lambda item:item[1],reverse=True)\n",
    "\n",
    "#choose the important ones--No.1-100\n",
    "WORDS=[x[0] for x in wordList]\n",
    "WORDS=WORDS[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high-frequency words:\n",
      "['this', 'food', 'NOT_the', 'NOT_to', 'us', 'place', 'order', 'no', 'servic', 'get', 'would', 'time', 'one', 'drink', 'like', 'ask', 'go', 'NOT_a', 'wait', 'tabl', 'NOT_', 'said', 'could', 'minut', 'came', 'got', 'bar', 'manag', 'back', 'good', 'NOT_and', 'told', 'restaur', 'peopl', 'NOT_it', 'even', 'friend', 'want', 'come', 'went', 'tri', 'look', 'server', 'room', 'night', 'vega', 'say', 'NOT_be', 'NOT_in', 'took', 'NOT_for', 'realli', 'walk', 'experi', 'check', 'NOT_i', 'NOT_of', 'first', 'NOT_this', 'NOT_even', 'hour', 'bartend', 'take', 'NOT_that', 'bad', 'NOT_back', 'way', 'waitress', 'anoth', 'custom', 'call', 'NOT_go', 'make', 'club', 'better', 'ever', 'worst', 'well', 'give', 'two', 'NOT_have', 'NOT_on', 'also', 'still', 'NOT_was', 'left', 'hotel', 'has', 'horribl', 'NOT_at', 'seat', 'great', 'his', 'eat', 'guy', 'disappoint', 'rude', 'know', 'around', 'staff']\n"
     ]
    }
   ],
   "source": [
    "#stop_words = set(stopwords.words('english')) \n",
    "stop_words = {'who', 'how', 'him', 'can', 'than', 'these', 'your', 'the', 'while', 'don', 'of', 'on', 'had', 'there', \"you've\", 'that', 'having', 'himself', \"mustn't\", 'same', 'are', \"won't\", 'then', 'itself', 'doing', 'from', 'both', 'where', 'wouldn', 'me', 'off', 'because', 'isn', \"you'd\", 'whom', 'mustn', 'is', 'themselves', 'no', 'very', 'up', 'd', 'ma', 'yours', 'been', 'ain', 'will', 'a', 'most', 'did', 'with', 'o', 'this', 'during', 'i', \"mightn't\", \"isn't\", 'being', 'couldn', 'them', 'not', 'such', 'her', 'some', 'only', \"didn't\", 'should', 'after', 'our', 'down', 'here', 'about', 'herself', \"hadn't\", 'but', 'he', 'an', 'am', 't', 'they', 'again', 'll', 've', 'didn', 'into', 'needn', 're', 'nor', \"couldn't\", 'above', 'all', \"should've\", 'm', 'other', 'below', \"she's\", 'just', 'between', 'hasn', 'own', 'yourselves', 'until', 'too', 'which', \"shouldn't\", 's', \"it's\", 'his', 'y', 'to', 'over', 'hadn', \"shan't\", 'does', 'weren', 'shouldn', 'under', \"aren't\", 'be', \"don't\", 'any', 'or', \"haven't\", 'she', 'aren', 'against', 'we', 'in', 'ourselves', 'have', 'won', \"wasn't\", 'wasn', 'you', 'what', 'mightn', \"weren't\", 'doesn', 'hers', 'myself', 'shan', 'before', 'more', \"wouldn't\", 'were', 'each', \"doesn't\", 'through', 'for', \"hasn't\", 'by', 'now', 'do', 'has', 'those', 'few', \"you'll\", 'once', 'it', 'their', 'further', \"you're\", 'my', 'at', 'when', 'yourself', 'why', 'as', 'was', 'and', 'out', \"needn't\", 'if', 'haven', 'its', 'theirs', \"that'll\", 'so', 'ours'}\n",
    "ps = PorterStemmer()\n",
    "stop_words_stem=[]\n",
    "for w in stop_words:\n",
    "    stop_words_stem.append(ps.stem(w))\n",
    "negWords=set([\"wouldn't\",'isn','wasn',\"weren't\", \"haven't\", \"hasn't\", \"couldn't\", \"isn't\", 'doesn','hasn',\"mustn't\", 'mightn', 'shan', 'no', \"wasn't\",'aren', \"didn't\", \"hadn't\",\"don't\",'nor',\"won't\",'weren',\"doesn't\",\"needn't\", 'shouldn',\"mightn't\",\"shan't\", 'wouldn',\"shouldn't\",'hadn'])\n",
    "negWords_stem=[]\n",
    "for w in negWords:\n",
    "    negWords_stem.append(ps.stem(w))\n",
    "d=set(['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z','à','{sigh}'])\n",
    "stopWords_stem=set(stop_words_stem)-set(negWords_stem)\n",
    "stopWords_stem.update(d)\n",
    "stopWords_stem.add('was')\n",
    "\n",
    "WORDS=[x for x in WORDS if x not in stopWords_stem]\n",
    "\n",
    "\n",
    "print('high-frequency words:')\n",
    "print(WORDS[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_wc_uni={}\n",
    "for i in range(len(wordList)):\n",
    "    if wordList[i][0] in WORDS:\n",
    "        dict_wc_uni[wordList[i][0]]=wordList[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom wordcloud import WordCloud\\nimport matplotlib.pyplot as plt\\n\\nwc = WordCloud(background_color = \\'White\\',width=400,height=300,scale=4,max_font_size=100)\\nwc.generate_from_frequencies(dict_wc)\\n\\nplt.figure(figsize=(20,12), dpi=1200)\\nplt.imshow(wc)\\nplt.axis(\"off\")\\nplt.show()\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wc = WordCloud(background_color = 'White',width=400,height=300,scale=4,max_font_size=100)\n",
    "wc.generate_from_frequencies(dict_wc)\n",
    "\n",
    "plt.figure(figsize=(20,12), dpi=1200)\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=list(dict_wc_uni.keys())\n",
    "frequency=list(dict_wc_uni.values())\n",
    "df_star1 = pd.DataFrame(np.nan,index=range(len(words)), columns=['loc'])\n",
    "df_star1['words']=words\n",
    "df_star1['frequency']=frequency\n",
    "df_star1=df_star1.loc[:,['words','frequency']]\n",
    "from pandas import DataFrame\n",
    "DataFrame.to_csv(df_star1,\"wc_uni_star1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select biwords due to frequency order\n",
    "BiFreq = reviewsTEXT_clean[0]['bigrams_freq']\n",
    "for ind in range(len(reviewsTEXT_clean)-1):\n",
    "    freqs = reviewsTEXT_clean[ind+1]['bigrams_freq']\n",
    "    BiFreq.update(freqs)\n",
    "    \n",
    "biwordList = sorted(BiFreq.items(),key=lambda item:item[1],reverse=True)\n",
    "\n",
    "#choose the important ones--No.1-100\n",
    "BIWORDS=[x[0] for x in biwordList]\n",
    "BIWORDS=BIWORDS[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it was',\n",
       " 'of the',\n",
       " 'we were',\n",
       " 'in the',\n",
       " 'to the',\n",
       " 'i was',\n",
       " 'this place',\n",
       " 'and the',\n",
       " 'and i',\n",
       " 'on the',\n",
       " 'at the',\n",
       " 'for the',\n",
       " 'for a',\n",
       " 'the food',\n",
       " 'it s',\n",
       " 'to get',\n",
       " 'to be',\n",
       " 'i m',\n",
       " 'we had',\n",
       " 'had to']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BIWORDS[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_wc_bi={}\n",
    "for i in range(len(biwordList)):\n",
    "    if biwordList[i][0] in BIWORDS:\n",
    "        dict_wc_bi[biwordList[i][0]]=biwordList[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "biwords=list(dict_wc_bi.keys())\n",
    "frequency=list(dict_wc_bi.values())\n",
    "df_star1 = pd.DataFrame(np.nan,index=range(len(biwords)), columns=['loc'])\n",
    "df_star1['biwords']=biwords\n",
    "df_star1['frequency']=frequency\n",
    "df_star1=df_star1.loc[:,['biwords','frequency']]\n",
    "from pandas import DataFrame\n",
    "DataFrame.to_csv(df_star1,\"wc_bi_star1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
