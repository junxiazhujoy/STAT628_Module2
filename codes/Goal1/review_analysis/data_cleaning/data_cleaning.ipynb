{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "import copy\n",
    "import nltk\n",
    "import stemming\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.stem import PorterStemmer\n",
    "from stemming.porter2 import stem\n",
    "from ngram import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [] \n",
    "cf = open('LVbars_reviewdf.csv','r')\n",
    "\n",
    "file = csv.DictReader(cf)\n",
    "#print(file.fieldnames)\n",
    "\n",
    "for x in file:\n",
    "    line = {'business_id':x['business_id'],'date':x['date'],'stars':x['stars'],'text':x['text']}\n",
    "    data.append(line)\n",
    "\n",
    "cf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_5star=[]\n",
    "data_4star=[]\n",
    "data_3star=[]\n",
    "data_2star=[]\n",
    "data_1star=[]\n",
    "for i in range(len(data)):\n",
    "    if data[i]['stars']=='5.0':\n",
    "        data_5star.append(data[i])\n",
    "    elif data[i]['stars']=='4.0':\n",
    "        data_4star.append(data[i])\n",
    "    elif data[i]['stars']=='3.0':\n",
    "        data_3star.append(data[i])\n",
    "    elif data[i]['stars']=='2.0':\n",
    "        data_2star.append(data[i])\n",
    "    else:\n",
    "        data_1star.append(data[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111769"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_5star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66133"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_4star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33822"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_3star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23239"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_2star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30884"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_1star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewslist = copy.deepcopy(data[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\$)|(\\*)|(\\%)|(\\_)|(\\=)|(\\#)|(\\&)|(\\~)|(\\@)\")#[^\\P{P}-]+\n",
    "REPLACE_WITH_SPACE = re.compile(\"(\\n)|(\\-)|(\\/)|(\\d)\")\n",
    "\n",
    "def preprocess_reviews(reviews):\n",
    "    reviews = REPLACE_NO_SPACE.sub(\"\", reviews)\n",
    "    reviews = REPLACE_WITH_SPACE.sub(\" \", reviews)\n",
    "    return reviews\n",
    "\n",
    "reviewsTEXT_clean = copy.deepcopy(reviewslist)\n",
    "\n",
    "for ind in range(len(reviewslist)):\n",
    "    texts = ''\n",
    "    texts = reviewslist[ind]['text']\n",
    "    texts = texts.lower()\n",
    "    texts = re.sub('n\\'t',' not', texts)\n",
    "    texts = re.sub('isnt','isn\\'t', texts)\n",
    "    texts = re.sub('wasnt','wasn\\'t', texts)\n",
    "    texts = re.sub('werent','weren\\'t', texts)\n",
    "    texts = re.sub('dont','don\\'t', texts)\n",
    "    texts = re.sub('doesnt','doesn\\'t', texts)\n",
    "    texts = re.sub('didnt','didn\\'t', texts)\n",
    "    texts = re.sub('hasnt','hasn\\'t', texts)\n",
    "    texts = re.sub('havent','haven\\'t', texts)\n",
    "    texts = re.sub('hadnt','hadn\\'t', texts)\n",
    "    texts = re.sub('mightnt','mightn\\'t', texts)\n",
    "    texts = re.sub('shouldnt','shouldn\\'t', texts)\n",
    "    texts = re.sub('isn','isn\\'t', texts)\n",
    "    texts = re.sub('wasn','wasn\\'t', texts)\n",
    "    texts = re.sub('weren','weren\\'t', texts)\n",
    "    texts = re.sub('don','don\\'t', texts)\n",
    "    texts = re.sub('doesn','doesn\\'t', texts)\n",
    "    texts = re.sub('didn','didn\\'t', texts)\n",
    "    texts = re.sub('hasn','hasn\\'t', texts)\n",
    "    texts = re.sub('haven','haven\\'t', texts)\n",
    "    texts = re.sub('hadn','hadn\\'t', texts)\n",
    "    texts = re.sub('mightn','mightn\\'t', texts)\n",
    "    texts = re.sub('shouldn','shouldn\\'t', texts)\n",
    "    \n",
    "    texts = re.sub('n\\'t',' not', texts)\n",
    "    \n",
    "    #add NOT_\n",
    "    pattern = r'\\.|\\;|\\!|\\?|\\,|\\)|\\(|\\:|\\'|\\\"|\\%'\n",
    "    list_text=re.split(pattern,texts)\n",
    "    \n",
    "    sent=''\n",
    "    for i in range(len(list_text)):\n",
    "        list_text[i] = re.sub('\\+','', list_text[i])\n",
    "        list_text[i] = re.sub('\\*','', list_text[i])\n",
    "        list_text[i] = re.sub('\\$','', list_text[i])\n",
    "        list_text[i] = re.sub('\\[','', list_text[i])\n",
    "        list_text[i] = re.sub('\\]','', list_text[i])\n",
    "        list_text[i] = re.sub('\\%','', list_text[i])\n",
    "        list_text[i] = re.sub('\\\\\\\\',' ', list_text[i])\n",
    "        matchObj1 = re.search(r'(.*)not (.*)',list_text[i])\n",
    "        matchObj2 = re.search(r'(.*) never (.*)',list_text[i])\n",
    "        \n",
    "        if matchObj1 != None :\n",
    "            sub=re.sub(r' ', \" notadd\", ' '+matchObj1.group(2))\n",
    "            list_text[i] = re.sub(matchObj1.group(2) ,sub, list_text[i])\n",
    "            list_text[i] = re.sub(r'not ','', list_text[i],1)\n",
    "            #list_text[i] = re.sub(',',' ', matchObj1.group(3))\n",
    "            sent = sent + list_text[i] + ' '\n",
    "        \n",
    "        elif matchObj2 != None :\n",
    "            sub=re.sub(r' ', \" notadd\", ' '+matchObj2.group(2))\n",
    "            list_text[i] = re.sub(matchObj2.group(2) ,sub, list_text[i])\n",
    "            list_text[i] = re.sub(r'never ','', list_text[i],1)\n",
    "            #list_text[i] = re.sub(',',' ', matchObj1.group(3))\n",
    "            sent = sent + list_text[i] + ' '\n",
    "        \n",
    "        else:\n",
    "            sent = sent + list_text[i] + ' '\n",
    "   \n",
    "    #print(sent)\n",
    "    #print(texts)\n",
    "    #print(texts)\n",
    "    text_clean=preprocess_reviews(sent)\n",
    "    #print(text_clean)\n",
    "    st = text_clean\n",
    "    st2 = \" \".join([stem(word) for word in st.split(\" \")])\n",
    "    text_clean = st2\n",
    "    \n",
    "    reviewsTEXT_clean[ind]['text']=text_clean\n",
    "    reviewsTEXT_clean[ind]['text_split']=text_clean.split()\n",
    "    reviewsTEXT_clean[ind]['freq']=nltk.FreqDist(reviewsTEXT_clean[ind]['text_split'])   \n",
    "    reviewsTEXT_clean[ind]['bigrams']=generate_ngrams(text_clean, 2)\n",
    "    reviewsTEXT_clean[ind]['bigrams_freq']=nltk.FreqDist(reviewsTEXT_clean[ind]['bigrams'])\n",
    "    \n",
    "#reviewsTEXT_clean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select words due to frequency order\n",
    "AllFreq = reviewsTEXT_clean[0]['freq']\n",
    "for ind in range(len(reviewsTEXT_clean)-1):\n",
    "    freqs = reviewsTEXT_clean[ind+1]['freq']\n",
    "    AllFreq.update(freqs)\n",
    "    \n",
    "wordList = sorted(AllFreq.items(),key=lambda item:item[1],reverse=True)\n",
    "\n",
    "#choose the important ones--No.1-100\n",
    "WORDS=[x[0] for x in wordList]\n",
    "WORDS=WORDS[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high-frequency words:\n",
      "['this', 'food', 'good', 'place', 'great', 'servic', 'order', 'notaddth', 'time', 'like', 'would', 'drink', 'restaur', 'friend', 'notadda', 'get', 'go', 'notaddto', 'us', 'one', 'vega', 'back', 'realli', 'tabl', 'come', 'tri', 'came', 'love', 'nice', 'best', 'menu', 'bar', 'server', 'well', 'no', 'notadd', 'burger', 'also', 'definit', 'got', 'could', 'look', 'fri', 'wait', 'notaddi', 'chees', 'delici', 'night', 'amaz', 'chicken', 'price', 'make', 'even', 'flavor', 'staff', 'ask', 'made', 'notaddit', 'meal', 'littl', 'tast', 'first', 'recommend', 'pretti', 'notaddand', 'dinner', 'want', 'experi', 'seat', 'notaddof', 'has', 'went', 'sauc', 'peopl', 'hour', 'eat', 'better', 'notaddon', 'beer', 'waiter', 'notaddfor', 'star', 'dish', 'awesom', 'guy', 'salad', 'thing', 'enjoy', 'everyth', 'notaddw', 'wine', 'day', 'notaddthat', 'steak', 'check', 'alway', 'ca', 'atmospher', 'notaddhav', 'visit']\n"
     ]
    }
   ],
   "source": [
    "#stop_words = set(stopwords.words('english')) \n",
    "stop_words = {'who', 'how', 'him', 'can', 'than', 'these', 'your', 'the', 'while', 'don', 'of', 'on', 'had', 'there', \"you've\", 'that', 'having', 'himself', \"mustn't\", 'same', 'are', \"won't\", 'then', 'itself', 'doing', 'from', 'both', 'where', 'wouldn', 'me', 'off', 'because', 'isn', \"you'd\", 'whom', 'mustn', 'is', 'themselves', 'no', 'very', 'up', 'd', 'ma', 'yours', 'been', 'ain', 'will', 'a', 'most', 'did', 'with', 'o', 'this', 'during', 'i', \"mightn't\", \"isn't\", 'being', 'couldn', 'them', 'not', 'such', 'her', 'some', 'only', \"didn't\", 'should', 'after', 'our', 'down', 'here', 'about', 'herself', \"hadn't\", 'but', 'he', 'an', 'am', 't', 'they', 'again', 'll', 've', 'didn', 'into', 'needn', 're', 'nor', \"couldn't\", 'above', 'all', \"should've\", 'm', 'other', 'below', \"she's\", 'just', 'between', 'hasn', 'own', 'yourselves', 'until', 'too', 'which', \"shouldn't\", 's', \"it's\", 'his', 'y', 'to', 'over', 'hadn', \"shan't\", 'does', 'weren', 'shouldn', 'under', \"aren't\", 'be', \"don't\", 'any', 'or', \"haven't\", 'she', 'aren', 'against', 'we', 'in', 'ourselves', 'have', 'won', \"wasn't\", 'wasn', 'you', 'what', 'mightn', \"weren't\", 'doesn', 'hers', 'myself', 'shan', 'before', 'more', \"wouldn't\", 'were', 'each', \"doesn't\", 'through', 'for', \"hasn't\", 'by', 'now', 'do', 'has', 'those', 'few', \"you'll\", 'once', 'it', 'their', 'further', \"you're\", 'my', 'at', 'when', 'yourself', 'why', 'as', 'was', 'and', 'out', \"needn't\", 'if', 'haven', 'its', 'theirs', \"that'll\", 'so', 'ours'}\n",
    "ps = PorterStemmer()\n",
    "stop_words_stem=[]\n",
    "for w in stop_words:\n",
    "    stop_words_stem.append(ps.stem(w))\n",
    "negWords=set([\"wouldn't\",'isn','wasn',\"weren't\", \"haven't\", \"hasn't\", \"couldn't\", \"isn't\", 'doesn','hasn',\"mustn't\", 'mightn', 'shan', 'no', \"wasn't\",'aren', \"didn't\", \"hadn't\",\"don't\",'nor',\"won't\",'weren',\"doesn't\",\"needn't\", 'shouldn',\"mightn't\",\"shan't\", 'wouldn',\"shouldn't\",'hadn'])\n",
    "negWords_stem=[]\n",
    "for w in negWords:\n",
    "    negWords_stem.append(ps.stem(w))\n",
    "d=set(['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z','à','{sigh}'])\n",
    "stopWords_stem=set(stop_words_stem)-set(negWords_stem)\n",
    "stopWords_stem.update(d)\n",
    "stopWords_stem.add('was')\n",
    "\n",
    "WORDS=[x for x in WORDS if x not in stopWords_stem]\n",
    "\n",
    "\n",
    "print('high-frequency words:')\n",
    "print(WORDS[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only keep those high-frequency words in text, so that when doing Vectorizer it will not 'out of memory'.\n",
    "for i in range(len(reviewsTEXT_clean)):\n",
    "    reviewsTEXT_clean[i]['text'] = ''\n",
    "    for ind in reviewsTEXT_clean[i]['text_split']:\n",
    "        if ind in WORDS:\n",
    "            reviewsTEXT_clean[i]['text'] = reviewsTEXT_clean[i]['text'] + ind +' '\n",
    "        else:\n",
    "            reviewsTEXT_clean[i]['text'] = reviewsTEXT_clean[i]['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of texts\n",
    "texts= [x['text'] for x in reviewsTEXT_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer = CountVectorizer(token_pattern=r'\\b[^\\d\\W]+\\b')\n",
    "vectorizer = CountVectorizer()\n",
    "#X = vectorizer.fit_transform(texts)\n",
    "transformer = TfidfTransformer()\n",
    "\n",
    "tfidf = transformer.fit_transform(vectorizer.fit_transform(texts))\n",
    "\n",
    "X=tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       this      food      good     place     great    servic     order  \\\n",
      "0  0.000000  0.000000  0.000000  0.000000  0.138305  0.066188  0.000000   \n",
      "1  0.046014  0.000000  0.000000  0.000000  0.053627  0.051328  0.000000   \n",
      "2  0.000000  0.163445  0.000000  0.000000  0.000000  0.185800  0.000000   \n",
      "3  0.000000  0.069287  0.000000  0.080437  0.082292  0.078764  0.000000   \n",
      "4  0.000000  0.000000  0.218080  0.000000  0.000000  0.000000  0.086867   \n",
      "5  0.083321  0.054506  0.064300  0.031638  0.064736  0.000000  0.076837   \n",
      "6  0.074797  0.000000  0.086584  0.000000  0.087171  0.000000  0.000000   \n",
      "7  0.000000  0.000000  0.152489  0.150062  0.153523  0.000000  0.000000   \n",
      "8  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "9  0.038159  0.037443  0.000000  0.043469  0.044471  0.000000  0.000000   \n",
      "\n",
      "   notaddth      time      like  ...   ladi  forgot  moment  polit  \\\n",
      "0  0.082628  0.000000  0.084770  ...    0.0     0.0     0.0    0.0   \n",
      "1  0.064077  0.000000  0.000000  ...    0.0     0.0     0.0    0.0   \n",
      "2  0.000000  0.000000  0.000000  ...    0.0     0.0     0.0    0.0   \n",
      "3  0.000000  0.098164  0.000000  ...    0.0     0.0     0.0    0.0   \n",
      "4  0.000000  0.000000  0.000000  ...    0.0     0.0     0.0    0.0   \n",
      "5  0.077351  0.000000  0.000000  ...    0.0     0.0     0.0    0.0   \n",
      "6  0.000000  0.000000  0.000000  ...    0.0     0.0     0.0    0.0   \n",
      "7  0.000000  0.000000  0.000000  ...    0.0     0.0     0.0    0.0   \n",
      "8  0.000000  0.000000  0.000000  ...    0.0     0.0     0.0    0.0   \n",
      "9  0.000000  0.000000  0.054514  ...    0.0     0.0     0.0    0.0   \n",
      "\n",
      "   notaddstar  pop  dollar  remind  notaddwrong  gone  \n",
      "0         0.0  0.0     0.0     0.0          0.0   0.0  \n",
      "1         0.0  0.0     0.0     0.0          0.0   0.0  \n",
      "2         0.0  0.0     0.0     0.0          0.0   0.0  \n",
      "3         0.0  0.0     0.0     0.0          0.0   0.0  \n",
      "4         0.0  0.0     0.0     0.0          0.0   0.0  \n",
      "5         0.0  0.0     0.0     0.0          0.0   0.0  \n",
      "6         0.0  0.0     0.0     0.0          0.0   0.0  \n",
      "7         0.0  0.0     0.0     0.0          0.0   0.0  \n",
      "8         0.0  0.0     0.0     0.0          0.0   0.0  \n",
      "9         0.0  0.0     0.0     0.0          0.0   0.0  \n",
      "\n",
      "[10 rows x 891 columns]\n"
     ]
    }
   ],
   "source": [
    "#vectorizer.get_feature_names()\n",
    "X_df=pd.DataFrame(X.A,columns=vectorizer.get_feature_names())\n",
    "\n",
    "Reviews_df=X_df[WORDS]\n",
    "print(Reviews_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "####add ngrams\n",
    "#select bigrams due to frequency order\n",
    "AllBiFreq = reviewsTEXT_clean[0]['bigrams_freq']\n",
    "for ind in range(len(reviewsTEXT_clean)-1):\n",
    "    Bifreqs = reviewsTEXT_clean[ind+1]['bigrams_freq']\n",
    "    AllBiFreq.update(Bifreqs)\n",
    "    \n",
    "BiwordList = sorted(AllBiFreq.items(),key=lambda item:item[1],reverse=True)\n",
    "BiwordList=BiwordList[0:500]\n",
    "\n",
    "#choose the important ones--No.1-100\n",
    "BiWORDS=[x[0] for x in BiwordList]\n",
    "Biwordfreq = {}\n",
    "for i in range(len(BiwordList)):\n",
    "    if len(list(set(BiwordList[i][0].split()).difference(stopWords_stem))) == 2:\n",
    "        Biwordfreq[BiwordList[i][0]]=BiwordList[i][1]\n",
    "\n",
    "BiWORDS=list(Biwordfreq.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(reviewsTEXT_clean)):\n",
    "    reviewsTEXT_clean[i]['text'] = ' '\n",
    "    for ind in reviewsTEXT_clean[i]['bigrams']:\n",
    "        if ind in BiWORDS:\n",
    "            reviewsTEXT_clean[i]['text'] = reviewsTEXT_clean[i]['text'] + ind +' '\n",
    "        else:\n",
    "            reviewsTEXT_clean[i]['text'] = reviewsTEXT_clean[i]['text']\n",
    "\n",
    "\n",
    "\n",
    "#create list of texts\n",
    "texts= [x['text'] for x in reviewsTEXT_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   back chees  back come  back custom  back happi  back high  back ice  \\\n",
      "0         0.0        0.0          0.0         0.0        0.0       0.0   \n",
      "1         0.0        0.0          0.0         0.0        0.0       0.0   \n",
      "2         0.0        0.0          0.0         0.0        0.0       0.0   \n",
      "3         0.0        0.0          0.0         0.0        0.0       0.0   \n",
      "4         0.0        0.0          0.0         0.0        0.0       0.0   \n",
      "5         0.0        0.0          0.0         0.0        0.0       0.0   \n",
      "6         0.0        0.0          0.0         0.0        0.0       0.0   \n",
      "7         0.0        0.0          0.0         0.0        0.0       0.0   \n",
      "8         0.0        0.0          0.0         0.0        0.0       0.0   \n",
      "9         0.0        0.0          0.0         0.0        0.0       0.0   \n",
      "\n",
      "   back las  back make  back next  back notaddat      ...        \\\n",
      "0       0.0        0.0        0.0            0.0      ...         \n",
      "1       0.0        0.0        0.0            0.0      ...         \n",
      "2       0.0        0.0        0.0            0.0      ...         \n",
      "3       0.0        0.0        0.0            0.0      ...         \n",
      "4       0.0        0.0        0.0            0.0      ...         \n",
      "5       0.0        0.0        0.0            0.0      ...         \n",
      "6       0.0        0.0        0.0            0.0      ...         \n",
      "7       0.0        0.0        0.0            0.0      ...         \n",
      "8       0.0        0.0        0.0            0.0      ...         \n",
      "9       0.0        0.0        0.0            0.0      ...         \n",
      "\n",
      "   vega notaddgo  vega notaddof  vega notaddon  vega realli  vega recommend  \\\n",
      "0            0.0            0.0            0.0          0.0             0.0   \n",
      "1            0.0            0.0            0.0          0.0             0.0   \n",
      "2            0.0            0.0            0.0          0.0             0.0   \n",
      "3            0.0            0.0            0.0          0.0             0.0   \n",
      "4            0.0            0.0            0.0          0.0             0.0   \n",
      "5            0.0            0.0            0.0          0.0             0.0   \n",
      "6            0.0            0.0            0.0          0.0             0.0   \n",
      "7            0.0            0.0            0.0          0.0             0.0   \n",
      "8            0.0            0.0            0.0          0.0             0.0   \n",
      "9            0.0            0.0            0.0          0.0             0.0   \n",
      "\n",
      "   vega sea  vega sushi  vega this  wolfgang puck  would definit  \n",
      "0       0.0         0.0        0.0            0.0            0.0  \n",
      "1       0.0         0.0        0.0            0.0            0.0  \n",
      "2       0.0         0.0        0.0            0.0            0.0  \n",
      "3       0.0         0.0        0.0            0.0            0.0  \n",
      "4       0.0         0.0        0.0            0.0            0.0  \n",
      "5       0.0         0.0        0.0            0.0            0.0  \n",
      "6       0.0         0.0        0.0            0.0            0.0  \n",
      "7       0.0         0.0        0.0            0.0            0.0  \n",
      "8       0.0         0.0        0.0            0.0            0.0  \n",
      "9       0.0         0.0        0.0            0.0            0.0  \n",
      "\n",
      "[10 rows x 366 columns]\n",
      "   come back  look like  notaddgo notaddback  notaddof notaddth  next time  \\\n",
      "0        0.0        0.0                  0.0                0.0        0.0   \n",
      "1        0.0        0.0                  0.0                0.0        0.0   \n",
      "2        0.0        0.0                  0.0                0.0        0.0   \n",
      "3        0.0        0.0                  0.0                0.0        0.0   \n",
      "4        0.0        0.0                  0.0                0.0        0.0   \n",
      "5        0.0        0.0                  0.0                0.0        0.0   \n",
      "6        0.0        0.0                  0.0                0.0        0.0   \n",
      "7        0.0        0.0                  0.0                0.0        0.0   \n",
      "8        0.0        0.0                  0.0                0.0        0.0   \n",
      "9        0.0        0.0                  0.0                0.0        0.0   \n",
      "\n",
      "   would definit  first time  ice cream  realli good  sushi samba  \\\n",
      "0            0.0         0.0        0.0          0.0          0.0   \n",
      "1            0.0         0.0        0.0          0.0          0.0   \n",
      "2            0.0         0.0        0.0          0.0          0.0   \n",
      "3            0.0         0.0        0.0          0.0          0.0   \n",
      "4            0.0         0.0        0.0          0.0          0.0   \n",
      "5            0.0         0.0        0.0          0.0          0.0   \n",
      "6            0.0         0.0        0.0          0.0          0.0   \n",
      "7            0.0         0.0        0.0          0.0          0.0   \n",
      "8            0.0         0.0        0.0          0.0          0.0   \n",
      "9            0.0         0.0        0.0          0.0          0.0   \n",
      "\n",
      "         ...          wolfgang puck  notaddwait notaddto  happi hour  \\\n",
      "0        ...                    0.0                  0.0         0.0   \n",
      "1        ...                    0.0                  0.0         0.0   \n",
      "2        ...                    0.0                  0.0         0.0   \n",
      "3        ...                    0.0                  0.0         0.0   \n",
      "4        ...                    0.0                  0.0         0.0   \n",
      "5        ...                    0.0                  0.0         0.0   \n",
      "6        ...                    0.0                  0.0         0.0   \n",
      "7        ...                    0.0                  0.0         0.0   \n",
      "8        ...                    0.0                  0.0         0.0   \n",
      "9        ...                    0.0                  0.0         0.0   \n",
      "\n",
      "   custom servic  long time  love this  pretti good  this place  \\\n",
      "0            0.0        0.0        0.0          0.0    0.000000   \n",
      "1            0.0        0.0        0.0          0.0    0.000000   \n",
      "2            0.0        0.0        0.0          0.0    0.000000   \n",
      "3            0.0        0.0        0.0          0.0    0.000000   \n",
      "4            0.0        0.0        0.0          0.0    0.000000   \n",
      "5            0.0        0.0        0.0          0.0    0.000000   \n",
      "6            0.0        0.0        0.0          0.0    0.000000   \n",
      "7            0.0        0.0        0.0          0.0    0.000000   \n",
      "8            0.0        0.0        0.0          0.0    0.000000   \n",
      "9            0.0        0.0        0.0          0.0    0.255809   \n",
      "\n",
      "   notaddit notaddwa  notaddon notaddth  \n",
      "0                0.0            0.00000  \n",
      "1                0.0            0.56342  \n",
      "2                0.0            0.00000  \n",
      "3                0.0            0.00000  \n",
      "4                0.0            0.00000  \n",
      "5                0.0            0.00000  \n",
      "6                0.0            0.00000  \n",
      "7                0.0            0.00000  \n",
      "8                0.0            0.00000  \n",
      "9                0.0            0.00000  \n",
      "\n",
      "[10 rows x 36 columns]\n",
      "       this      food      good     place     great    servic     order  \\\n",
      "0  0.000000  0.000000  0.000000  0.000000  0.138305  0.066188  0.000000   \n",
      "1  0.046014  0.000000  0.000000  0.000000  0.053627  0.051328  0.000000   \n",
      "2  0.000000  0.163445  0.000000  0.000000  0.000000  0.185800  0.000000   \n",
      "3  0.000000  0.069287  0.000000  0.080437  0.082292  0.078764  0.000000   \n",
      "4  0.000000  0.000000  0.218080  0.000000  0.000000  0.000000  0.086867   \n",
      "5  0.083321  0.054506  0.064300  0.031638  0.064736  0.000000  0.076837   \n",
      "6  0.074797  0.000000  0.086584  0.000000  0.087171  0.000000  0.000000   \n",
      "7  0.000000  0.000000  0.152489  0.150062  0.153523  0.000000  0.000000   \n",
      "8  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "9  0.038159  0.037443  0.000000  0.043469  0.044471  0.000000  0.000000   \n",
      "\n",
      "   notaddth      time      like     ...       notaddwait notaddto  happi hour  \\\n",
      "0  0.082628  0.000000  0.084770     ...                       0.0         0.0   \n",
      "1  0.064077  0.000000  0.000000     ...                       0.0         0.0   \n",
      "2  0.000000  0.000000  0.000000     ...                       0.0         0.0   \n",
      "3  0.000000  0.098164  0.000000     ...                       0.0         0.0   \n",
      "4  0.000000  0.000000  0.000000     ...                       0.0         0.0   \n",
      "5  0.077351  0.000000  0.000000     ...                       0.0         0.0   \n",
      "6  0.000000  0.000000  0.000000     ...                       0.0         0.0   \n",
      "7  0.000000  0.000000  0.000000     ...                       0.0         0.0   \n",
      "8  0.000000  0.000000  0.000000     ...                       0.0         0.0   \n",
      "9  0.000000  0.000000  0.054514     ...                       0.0         0.0   \n",
      "\n",
      "   custom servic  long time  love this  pretti good  this place  \\\n",
      "0            0.0        0.0        0.0          0.0    0.000000   \n",
      "1            0.0        0.0        0.0          0.0    0.000000   \n",
      "2            0.0        0.0        0.0          0.0    0.000000   \n",
      "3            0.0        0.0        0.0          0.0    0.000000   \n",
      "4            0.0        0.0        0.0          0.0    0.000000   \n",
      "5            0.0        0.0        0.0          0.0    0.000000   \n",
      "6            0.0        0.0        0.0          0.0    0.000000   \n",
      "7            0.0        0.0        0.0          0.0    0.000000   \n",
      "8            0.0        0.0        0.0          0.0    0.000000   \n",
      "9            0.0        0.0        0.0          0.0    0.255809   \n",
      "\n",
      "   notaddit notaddwa  notaddon notaddth  business_id  \n",
      "0                0.0            0.00000       125478  \n",
      "1                0.0            0.56342         9439  \n",
      "2                0.0            0.00000        96069  \n",
      "3                0.0            0.00000        41738  \n",
      "4                0.0            0.00000       117028  \n",
      "5                0.0            0.00000       130178  \n",
      "6                0.0            0.00000       130178  \n",
      "7                0.0            0.00000       103432  \n",
      "8                0.0            0.00000        34159  \n",
      "9                0.0            0.00000        97837  \n",
      "\n",
      "[10 rows x 928 columns]\n"
     ]
    }
   ],
   "source": [
    "#vectorizer = CountVectorizer(token_pattern=r'\\b[^\\d\\W]+\\b')\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "#X = vectorizer.fit_transform(texts)\n",
    "transformer = TfidfTransformer()\n",
    "\n",
    "tfidf = transformer.fit_transform(vectorizer.fit_transform(texts))\n",
    "\n",
    "X_ngrams=tfidf\n",
    "X_ngrams_df=pd.DataFrame(X_ngrams.A,columns = vectorizer.get_feature_names())\n",
    "print(X_ngrams_df.head(10))\n",
    "Reviews_ngrams_df=X_ngrams_df[list(set(Biwordfreq.keys()).intersection(set(vectorizer.get_feature_names())))]\n",
    "Reviews_ngrams_df\n",
    "print(Reviews_ngrams_df.head(10))\n",
    "\n",
    "Reviews_df=pd.concat([Reviews_df,Reviews_ngrams_df],axis=1)\n",
    "\n",
    "#Add Business ID in the data frame\n",
    "businessIDs=[x['business_id'] for x in reviewsTEXT_clean]\n",
    "Reviews_df['business_id']=businessIDs\n",
    "print(Reviews_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "DataFrame.to_csv(Reviews_df,\"clean.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
